{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Stanza parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 22:34:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b3590bc3944fafa9b06baa00e2aaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 22:34:53 INFO: Downloaded file to /home/pricie/bach/stanza_resources/resources.json\n",
      "2025-08-30 22:35:00 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2025-08-30 22:35:01 INFO: Using device: cuda\n",
      "2025-08-30 22:35:01 INFO: Loading: tokenize\n",
      "2025-08-30 22:35:05 INFO: Loading: mwt\n",
      "2025-08-30 22:35:05 INFO: Loading: pos\n",
      "2025-08-30 22:35:07 INFO: Loading: lemma\n",
      "2025-08-30 22:35:07 INFO: Loading: depparse\n",
      "2025-08-30 22:35:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "# stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "        'He has been running for 5 years.',\n",
    "        #  'The boss runs the company.',\n",
    "        #  'The company is run by the boss', \n",
    "        #  'The company is run in the dark.', \n",
    "        #  'He runs in the jungle.',\n",
    "        #  'The roads run through the city.',\n",
    "        #  'He runs his finger through his hair.',\n",
    "        #  'The computer runs fast.',\n",
    "        #  'The car runs really fast.'\n",
    "         ]\n",
    "\n",
    "# sents = ['MISS NORMAN : Will you do me the honour to meet me at the bridgehead at half-past nine practically at once ?']\n",
    "target = 'miss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: He has been running for 5 years.\n",
      "word: He\tlemma: he\tpos: PRON\tid: 1\thead id: 4\thead: running\tdeprel: nsubj\n",
      "word: has\tlemma: have\tpos: AUX\tid: 2\thead id: 4\thead: running\tdeprel: aux\n",
      "word: been\tlemma: be\tpos: AUX\tid: 3\thead id: 4\thead: running\tdeprel: aux\n",
      "word: running\tlemma: run\tpos: VERB\tid: 4\thead id: 0\thead: root\tdeprel: root\n",
      "word: for\tlemma: for\tpos: ADP\tid: 5\thead id: 7\thead: years\tdeprel: case\n",
      "word: 5\tlemma: 5\tpos: NUM\tid: 6\thead id: 7\thead: years\tdeprel: nummod\n",
      "word: years\tlemma: year\tpos: NOUN\tid: 7\thead id: 4\thead: running\tdeprel: obl\n",
      "word: .\tlemma: .\tpos: PUNCT\tid: 8\thead id: 4\thead: running\tdeprel: punct\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    doc = nlp(sent) # run annotation over a sentence\n",
    "    print('sentence:', sent)\n",
    "    # print(doc)\n",
    "    # print(doc.entities)\n",
    "    print(*[f'word: {word.text}\\tlemma: {word.lemma}\\tpos: {word.pos}\\tid: {word.id}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "    print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from collections import defaultdict\n",
    "\n",
    "sents = ['The boss runs the company.',\n",
    "         'The company is run by the boss', \n",
    "         'The company is run in the dark.', \n",
    "         'He runs in the jungle.',\n",
    "         'The roads run through the city.',\n",
    "         'He runs his finger through his hair.',\n",
    "         'The computer runs fast.',\n",
    "         'The car runs really fast.']\n",
    "target = 'run'\n",
    "\n",
    "# 1. load the English pipeline (tokeniser-POS-lemma-dependency)\n",
    "nlp = stanza.Pipeline(\n",
    "        'en', processors='tokenize,pos,lemma,depparse',\n",
    "        tokenize_no_ssplit=True,  # treat each string as a single sentence\n",
    "        verbose=False)\n",
    "\n",
    "results = []               # (sent_id, dep_lemma, deprel)\n",
    "for sent_id, text in enumerate(sents, 1):\n",
    "    doc = nlp(text)\n",
    "    sent = doc.sentences[0]               # exactly one per string\n",
    "    for w in sent.words:                  # iterate over tokens/words\n",
    "        if w.lemma == target:             # <- our target lemma\n",
    "            head_id = w.id\n",
    "            # collect *immediate* dependents of this “run”\n",
    "            for d in sent.words:\n",
    "                if d.head == head_id:\n",
    "                    results.append((sent_id, d.lemma, d.deprel))\n",
    "\n",
    "# pretty-print\n",
    "for sent_id, lem, rel in results:\n",
    "    print(f'S{sent_id}: {lem:<10}  {rel}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = ['The boss runs the company.',\n",
    "#          'The company is run by the boss', \n",
    "#          'The company is run in the dark.', \n",
    "#          'He runs in the jungle.',\n",
    "#          'The roads run through the city.',\n",
    "#          'He runs his finger through his hair.',\n",
    "#          'The computer runs fast.',\n",
    "#          'The car runs really fast.'\n",
    "#          ]\n",
    "\n",
    "# sents = [\n",
    "#     \"Freedom is priceless.\",\n",
    "#     \"She fought for freedom during the revolution.\",\n",
    "#     \"The court finally granted him the freedom to speak openly.\",\n",
    "#     \"Within the classroom, freedom of thought nurtures creativity.\",\n",
    "#     \"The towering bronze sculpture, Freedom, dominates the plaza.\",\n",
    "#     \"After the last exam, the students burst outside in pure freedom.\",\n",
    "#     \"Digital tracking can quietly erode freedom online.\",\n",
    "#     \"We debated whether freedom or security mattered more.\",\n",
    "#     \"Without self-control, freedom often collapses into chaos.\",\n",
    "#     \"He inhaled deeply, freedom flooding his lungs at the prison gates.\"\n",
    "# ]\n",
    "\n",
    "# sents = [\n",
    "#     \"The table shook during the earthquake.\",\n",
    "#     \"She carved her initials into the wooden table.\",\n",
    "#     \"After dinner, they sat around the table and talked for hours.\",\n",
    "#     \"The architect presented a glass table as the room's centerpiece.\",\n",
    "#     \"Please table the motion until next week’s meeting.\",\n",
    "#     \"We sorted the data into a table for easier comparison.\",\n",
    "#     \"The cat leapt onto the table, knocking over a vase.\",\n",
    "#     \"Negotiators agreed to table further discussion until sunrise.\",\n",
    "#     \"Beneath the table, a hidden drawer contained old photographs.\",\n",
    "#     \"A picnic table stood alone under the oak tree.\"\n",
    "# ]\n",
    "\n",
    "sents = [\n",
    "    \"This article is interesting.\",\n",
    "    \"An interesting twist changed the plot completely.\",\n",
    "    \"He found the lecture interesting despite the late hour.\",\n",
    "    \"Someone interesting moved into the apartment next door.\",\n",
    "    \"The most interesting of the artifacts was the jade mask.\",\n",
    "    \"Keep your questions interesting and concise.\",\n",
    "    \"They made the workshop interesting by adding hands-on demos.\",\n",
    "    \"What I find interesting is how quickly trends shift.\",\n",
    "    \"Do you have anything interesting to read on the train?\",\n",
    "    \"Interesting, she thought, how silence can speak louder than words.\"\n",
    "]\n",
    "\n",
    "\n",
    "TARGET = 'interesting' \n",
    "\n",
    "MAX_DEPTH  = 2             # you can pass (1,), (2,), (1,2,3) …\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "nlp = stanza.Pipeline(\n",
    "        \"en\",\n",
    "        processors=\"tokenize,pos,lemma,depparse\",\n",
    "        tokenize_no_ssplit=True,\n",
    "        verbose=False)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "def collect_connected(sent, target_lemma, max_depth):\n",
    "    \"\"\"\n",
    "    Return {depth: [(lemma, path)]} where 'path' is a string like\n",
    "    '↓obj' or '↑nsubj:pass > ↓obl' showing the route from the target\n",
    "    to the node.  Traversal is undirected, up to max_depth edges.\n",
    "    \"\"\"\n",
    "    id2word   = {w.id: w for w in sent.words}\n",
    "    neighbours = defaultdict(list)                  # id -> [(word, label)]\n",
    "\n",
    "    # build bidirectional edges\n",
    "    for w in sent.words:\n",
    "        if w.head == 0:                             # ROOT has no parent\n",
    "            continue\n",
    "        head = id2word[w.head]\n",
    "        neighbours[w.id].append((head, f\"↑{w.deprel}\"))   # child -> parent\n",
    "        neighbours[head.id].append((w, f\"↓{w.deprel}\"))   # parent -> child\n",
    "\n",
    "    result = defaultdict(list)                      # depth -> [(lemma, path)]\n",
    "    for w in sent.words:\n",
    "        if w.lemma != target_lemma:\n",
    "            continue                                # other lemmas not our start\n",
    "        q = deque([(w, 0, [])])                     # node, depth, path so far\n",
    "        visited = {w.id}\n",
    "        while q:\n",
    "            node, d, path = q.popleft()\n",
    "            if d == max_depth:                      # stop expanding beyond limit\n",
    "                continue\n",
    "            for nb, rel in neighbours[node.id]:\n",
    "                if nb.id in visited:\n",
    "                    continue\n",
    "                nd     = d + 1\n",
    "                npath  = path + [rel]\n",
    "                result[nd].append((nb.lemma, \" > \".join(npath)))\n",
    "                visited.add(nb.id)\n",
    "                q.append((nb, nd, npath))\n",
    "    return result\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "all_hits = defaultdict(lambda: defaultdict(list))   # sent_id -> depth -> items\n",
    "for sid, text in enumerate(sents, 1):\n",
    "    sent = nlp(text).sentences[0]\n",
    "    dep_map = collect_connected(sent, TARGET, MAX_DEPTH)\n",
    "    for d, items in dep_map.items():\n",
    "        all_hits[sid][d].extend(items)\n",
    "\n",
    "# --- demo print ---------------------------------------------------- #\n",
    "for sid in sorted(all_hits):\n",
    "    print(f\"\\nSentence {sid}: {sents[sid-1]}\")\n",
    "    for d in sorted(all_hits[sid]):\n",
    "        print(f\"  depth {d}:\")\n",
    "        for lem, rel_path in all_hits[sid][d]:\n",
    "            print(f\"      {lem:<10}  {rel_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "pattern = re.compile(\n",
    "    r'([^\\t]+)\\t'      # word form\n",
    "    r'([^\\t]+)\\t'      # lemma\n",
    "    r'([^\\t])[^\\t]*\\t' # POS (UPOS or XPOS)\n",
    "    r'([^\\t]+)\\t'      # ID\n",
    "    r'([^\\t]+)\\t'      # HEAD\n",
    "    r'([^\\t]+)'        # DEPREL\n",
    ")\n",
    "\n",
    "target_lemma = 'air'\n",
    "target_pos = 'N'\n",
    "\n",
    "# # All\n",
    "period = '1750-1799'\n",
    "corpus_folder = f'/home/volt/bach/pilot_data/RSC/1750-1799_che'\n",
    "output_folder = Path(f'/home/volt/bach/SynFlow/case_studies/RSC_air_water_acid/output')\n",
    "output_folder_lemma = output_folder / f'{target_lemma}-{target_pos}-{period}'\n",
    "output_explorer = f'{output_folder_lemma}/Explorer'\n",
    "output_embedding = f'{output_folder_lemma}/Embedding'\n",
    "\n",
    "# Decades\n",
    "# period = '1790'\n",
    "# corpus_folder = f'/home/volt/bach/pilot_data/RSC/1750-1799_che_decades/{period}'\n",
    "# output_folder_lemma = f'/home/volt/bach/SynFlow/output/{target_lemma}-{target_pos}-{period}'\n",
    "# visualisation_folder = f'/home/volt/bach/SynFlow/visualisation/{target_lemma}-{target_pos}-{period}'\n",
    "\n",
    "# Half decades\n",
    "# period = '1770-1774'\n",
    "# corpus_folder = f'/home/volt/bach/pilot_data/RSC/1750-1799_che_half_decades/{period}'\n",
    "# output_folder_lemma = f'/home/volt/bach/SynFlow/output/{target_lemma}-{target_pos}-{period}'\n",
    "# output_explorer = f'{output_folder_lemma}/Explorer'\n",
    "# output_embedding = f'{output_folder_lemma}/Embedding'\n",
    "\n",
    "if not os.path.exists(output_explorer):\n",
    "    os.makedirs(output_explorer)\n",
    "\n",
    "if not os.path.exists(output_embedding):\n",
    "    os.makedirs(output_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SynFlow.Embedding.get_embeddings import build_embeddings\n",
    "\n",
    "slot_mode = 'mult'\n",
    "tok_mode='mult'\n",
    "df_emb = build_embeddings(\n",
    "    df_templates=pd.read_csv(f'{output_embedding}/{target_lemma}_samples_{n}_slots.csv', index_col=0), # df_slots,\n",
    "    type_embedding_path='/home/volt/bach/SynFlow/input/type_embedding/coha_10_20_w2v.csv',\n",
    "    dims=300,\n",
    "    slot_mode=slot_mode,\n",
    "    tok_mode=tok_mode,\n",
    "    out_embedding=f'{output_embedding}/{target_lemma}_samples_{n}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/volt/bach/SynFlow/type_embedding/coha_10_20_w2v.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SynFlow.Embedding.compute_dist import compute_cosine_distmtx\n",
    "\n",
    "# Example usage:\n",
    "df_emb = pd.read_csv(f\"{output_embedding}/{target_lemma}_samples_{n}_{slot_mode}_{tok_mode}_embedding.csv\", index_col=0)\n",
    "dist_df = compute_cosine_distmtx(df_emb)\n",
    "dist_df.to_csv(f\"{output_embedding}/{target_lemma}_samples_{n}_distance_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the cosine similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_np(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors using NumPy.\n",
    "\n",
    "    Args:\n",
    "        vec1 (numpy.ndarray or list): The first vector.\n",
    "        vec2 (numpy.ndarray or list): The second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two vectors.\n",
    "               Returns 0 if either vector has a magnitude of zero.\n",
    "    \"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0  # Or handle as an error, depending on desired behavior\n",
    "    else:\n",
    "        return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Example Usage:\n",
    "vector_a = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "vector_b = [3, 5, 7, 9, 11, 13, 15, 17]\n",
    "\n",
    "vector_c = [5, 6, 7, 8, 1, 2, 3, 4]\n",
    "vector_d = [11, 13, 15, 17, 3, 5, 7, 9]\n",
    "\n",
    "# vector_a = [1, 1, 1, 1, 2, 2, 2, 2]\n",
    "# vector_b = [1, 1, 1, 1, 2, 2, 2, 2]\n",
    "\n",
    "similarity = cosine_similarity_np(vector_a, vector_b)\n",
    "print(f\"Cosine Similarity (NumPy): {similarity}\")\n",
    "\n",
    "similarity = cosine_similarity_np(vector_c, vector_d)\n",
    "print(f\"Cosine Similarity (NumPy): {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from SynFlow.Explorer.get_contexts import get_contexts\n",
    "import re\n",
    "pattern = re.compile(\n",
    "    r'([^\\t]+)\\t'      # FORM\n",
    "    r'([^\\t]+)\\t'      # LEMMA\n",
    "    r'([^\\t])[^\\t]*\\t' # POS\n",
    "    r'([^\\t]+)\\t'      # ID\n",
    "    r'([^\\t]+)\\t'      # HEAD\n",
    "    r'([^\\t]+)'        # DEPREL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "slots_df = pd.read_csv(f\"{output_embedding}/{target_lemma}_samples_{n}_slots.csv\", index_col=0)\n",
    "\n",
    "# Now attach contexts:\n",
    "context_df = get_contexts(\n",
    "    slots_df=slots_df,\n",
    "    corpus_path=\"/home/volt/bach/pilot_data/COHA/10_20_parsed_1_SPOS\",\n",
    "    pattern=pattern,\n",
    "    output_path=f\"{output_embedding}/{target_lemma}_samples_{n}_contexts.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hdbscan\n",
    "\n",
    "def hdbscan_clustering(dist_df: pd.DataFrame,\n",
    "                          min_cluster_size: int = 5,\n",
    "                          min_samples: int = None,\n",
    "                          cluster_selection_epsilon: float = 0.0,\n",
    "                          cluster_selection_method: str = 'eom',\n",
    "                         ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a precomputed distance matrix `dist_df` (square DataFrame indexed and\n",
    "    columned by token IDs), run HDBSCAN (metric='precomputed') and return a new\n",
    "    DataFrame with two columns:\n",
    "      • 'token'   : the token ID (index of dist_df)\n",
    "      • 'cluster' : the HDBSCAN cluster label (-1 for noise)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dist_df : pd.DataFrame\n",
    "        Square distance matrix (n × n), index and columns are identical token IDs.\n",
    "    min_cluster_size : int, default=5\n",
    "        The minimum size of clusters; see HDBSCAN docs.\n",
    "    min_samples : int or None, default=None\n",
    "        Controls how conservative the clustering is; if None, it defaults to\n",
    "        min_cluster_size.\n",
    "    cluster_selection_epsilon : float, default=0.0\n",
    "        A distance threshold: clusters below this distance can be split off.\n",
    "    cluster_selection_method : {'eom','leaf'}, default='eom'\n",
    "        How to select clusters from the condensed tree.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns ['id','cluster'], index 0..n-1\n",
    "    \"\"\"\n",
    "    # Extract the numpy distance matrix\n",
    "    D = dist_df.values\n",
    "    # Initialize HDBSCAN with precomputed distances\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        metric='precomputed',\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        cluster_selection_method=cluster_selection_method\n",
    "    )\n",
    "    # Fit on the distance matrix\n",
    "    clusterer.fit(D)\n",
    "    labels = clusterer.labels_  # array of length n, -1 means noise\n",
    "\n",
    "    # Prefix each label with 'c'\n",
    "    clusters_prefixed = [f\"c{lab}\" for lab in labels]\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'id': dist_df.index,\n",
    "        'clusters': clusters_prefixed\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suppose you already computed dist_df (square DataFrame with token IDs as index & columns)\n",
    "dist_df = pd.read_csv(fr'{output_embedding}/{target_lemma}_samples_{n}_distance_matrix.csv', index_col=0)\n",
    "\n",
    "# Cluster with HDBSCAN\n",
    "cluster_df = hdbscan_clustering(\n",
    "    dist_df,\n",
    "    min_cluster_size=10,\n",
    "    min_samples=10\n",
    ")\n",
    "\n",
    "# Save to CSV if desired\n",
    "cluster_df.to_csv(fr'{output_embedding}/{target_lemma}_samples_{n}_clusters.csv', index=False)\n",
    "\n",
    "# Merge to context\n",
    "context_df = pd.read_csv(fr'{output_embedding}/{target_lemma}_samples_{n}_contexts.csv', index_col=0)\n",
    "cluster_context_df = context_df.merge(cluster_df, left_index=True, right_on='id')\n",
    "\n",
    "# Save to CSV if desired\n",
    "cluster_context_df.to_csv(fr'{output_embedding}/{target_lemma}_samples_{n}_clusters_contexts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Coordinates with tsne, umap, mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import SynFlow.Embedding.get_coordinates\n",
    "importlib.reload(SynFlow.Embedding.get_coordinates)\n",
    "from SynFlow.Embedding.get_coordinates import get_token_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "dist_df = pd.read_csv(f'{output_embedding}/{target_lemma}_samples_{n}_distance_matrix.csv', index_col=0)\n",
    "coord_tsne = get_token_coordinates(lemma=f'{target_lemma}_samples', dist_df=dist_df, method='tsne', perplexity=30, output_path=output_embedding, n = n)\n",
    "# coord_mds  = get_token_coordinates(lemma=f'{target_lemma}_samples', dist_df=dist_df, method='mds', max_iter=300, output_path=visualisation_folder, n = n)\n",
    "coord_umap = get_token_coordinates(lemma=f'{target_lemma}_samples', dist_df=dist_df, method='umap', n_neighbors=30, min_dist=0.1, output_path=output_embedding, n = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import SynFlow.Embedding.visualisation\n",
    "importlib.reload(SynFlow.Embedding.visualisation)\n",
    "from SynFlow.Embedding.visualisation import get_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_coords = fr'{output_embedding}/{target_lemma}_samples_{n}_tsne.csv'\n",
    "input_ctxs = fr'{output_embedding}/{target_lemma}_samples_{n}_clusters_contexts.csv'\n",
    "\n",
    "get_token_ids(input_coords, input_ctxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
